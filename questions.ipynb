{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import necessary NLTK data for tokenization and stopword removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /home/razvansavin/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/razvansavin/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import required libraries and set constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import os\n",
    "import math\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "FILE_MATCHES = 1\n",
    "SENTENCE_MATCHES = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function to load text files from a given directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_files(directory):\n",
    "    \"\"\"\n",
    "    Load all text files from a given directory.\n",
    "\n",
    "    Parameters:\n",
    "    directory (str): The directory containing the text files.\n",
    "\n",
    "    Returns:\n",
    "    dict: A dictionary mapping filenames to their contents.\n",
    "    \"\"\"\n",
    "    files = {}\n",
    "    # Iterate over each file in the directory\n",
    "    for filename in os.listdir(directory):\n",
    "        # Check if the file has a \".txt\" extension\n",
    "        if filename.endswith(\".txt\"):\n",
    "            with open(os.path.join(directory, filename), encoding=\"utf-8\") as file:\n",
    "                files[filename] = file.read()\n",
    "    return files\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function to tokenize documents, converting to lowercase and removing punctuation and stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(document):\n",
    "    \"\"\"\n",
    "    Tokenize a document into words, converting to lowercase and removing punctuation and stopwords.\n",
    "\n",
    "    Parameters:\n",
    "    document (str): The document to tokenize.\n",
    "\n",
    "    Returns:\n",
    "    list: A list of words in the document.\n",
    "    \"\"\"\n",
    "    # Convert the entire document to lowercase\n",
    "    document = document.lower()\n",
    "    # Tokenize the document into a list of words\n",
    "    tokens = nltk.tokenize.word_tokenize(document)\n",
    "    # Get the set of English stopwords\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    # Create a new list of tokens without stopwords and punctuation\n",
    "    new_tokens = [\n",
    "        token for token in tokens if token.isalpha() and token not in stop_words and token not in string.punctuation\n",
    "    ]\n",
    "    return new_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function to compute Inverse Document Frequency (IDF) values for each word in the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_idfs(documents):\n",
    "    \"\"\"\n",
    "    Compute Inverse Document Frequency (IDF) values for each word in the corpus.\n",
    "\n",
    "    Parameters:\n",
    "    documents (dict): A dictionary mapping document names to a list of their words.\n",
    "\n",
    "    Returns:\n",
    "    dict: A dictionary mapping words to their IDF values.\n",
    "    \"\"\"\n",
    "    words = set()\n",
    "    # Iterate through each document in the corpus\n",
    "    for file_name in documents:\n",
    "        # Add all words in the current document to the set of unique words\n",
    "        words.update(documents[file_name])\n",
    "    # Dict to store the IDF values for each word in the corpus\n",
    "    idfs = {}\n",
    "    total_documents = len(documents)\n",
    "    # Calculate IDF for each word in the corpus\n",
    "    for word in words:\n",
    "        num_documents_containing_word = sum(word in documents[file_name] for file_name in documents)\n",
    "        idf = math.log(total_documents / num_documents_containing_word)\n",
    "        idfs[word] = idf\n",
    "    return idfs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function to find the top file matches for a query based on TF-IDF scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_files(query, files, idfs, n):\n",
    "    \"\"\"\n",
    "    Find the top file matches for a query based on TF-IDF scores.\n",
    "\n",
    "    Parameters:\n",
    "    query (set): A set of query words.\n",
    "    files (dict): A dictionary mapping filenames to a list of their words.\n",
    "    idfs (dict): A dictionary mapping words to their IDF values.\n",
    "    n (int): The number of top files to return.\n",
    "\n",
    "    Returns:\n",
    "    list: A list of filenames of the top n matching files.\n",
    "    \"\"\"\n",
    "    file_scores = []\n",
    "    # Iterate over each file and its corresponding words\n",
    "    for file_name, words in files.items():\n",
    "        # Initialize the tf-idf score for the current file\n",
    "        tf_idf = 0\n",
    "        # Iterate over each word in the query\n",
    "        for word in query:\n",
    "            tf = words.count(word)\n",
    "            idf = idfs.get(word, 0)\n",
    "            tf_idf += tf * idf\n",
    "        file_scores.append((file_name, tf_idf))\n",
    "    # Sort the file_scores list in descending order based on the tf-idf score\n",
    "    file_scores.sort(key=lambda idf_score: idf_score[1], reverse=True)\n",
    "    # Get the top n files based on their tf-idf scores\n",
    "    top_n_files = [filename for filename, tf_idf in file_scores[:n]]\n",
    "    return top_n_files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function to find the top sentence matches for a query based on matching word measure and query term density"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_sentences(query, sentences, idfs, n):\n",
    "    \"\"\"\n",
    "    Find the top sentence matches for a query based on matching word measure and query term density.\n",
    "\n",
    "    Parameters:\n",
    "    query (set): A set of query words.\n",
    "    sentences (dict): A dictionary mapping sentences to a list of their words.\n",
    "    idfs (dict): A dictionary mapping words to their IDF values.\n",
    "    n (int): The number of top sentences to return.\n",
    "\n",
    "    Returns:\n",
    "    list: A list of the top n matching sentences.\n",
    "    \"\"\"\n",
    "    sentence_scores = []\n",
    "    # Iterate over each sentence and its corresponding words\n",
    "    for sentence, words in sentences.items():\n",
    "        # Calculate the matching word measure for the current sentence\n",
    "        matching_word_measure = sum(idfs.get(word, 0) for word in query if word in words)\n",
    "        # Calculate the query term density for the current sentence\n",
    "        query_term_density = sum(word in query for word in words) / len(words)\n",
    "        if len(sentence) > 0:\n",
    "            sentence_scores.append((sentence, matching_word_measure, query_term_density))\n",
    "    # Sort the sentence_scores list in descending order based on the matching word measure and query term density\n",
    "    sentence_scores.sort(key=lambda x: (x[1], x[2]), reverse=True)\n",
    "    # Get the top n sentences based on their scores\n",
    "    top_sentences = [score[0] for score in sentence_scores[:n]]\n",
    "    return top_sentences[:n]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main function to execute the question-answering system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(corpus_dir, queries):\n",
    "    \"\"\"\n",
    "    Main function to execute the question-answering system.\n",
    "    - Calculates IDF values for the corpus files.\n",
    "    - Iterates over each query to find answers.\n",
    "    - Determines top file matches based on TF-IDF.\n",
    "    - Extracts sentences from the top files.\n",
    "    - Calculates IDF values for the sentences.\n",
    "    - Determines top sentence matches.\n",
    "\n",
    "    Parameters:\n",
    "    corpus_dir (str): The directory containing the text files.\n",
    "    queries (list): A list of query strings.\n",
    "    \"\"\"\n",
    "    files = load_files(corpus_dir)\n",
    "    file_words = {filename: tokenize(files[filename]) for filename in files}\n",
    "    file_idfs = compute_idfs(file_words)\n",
    "\n",
    "    for query in queries:\n",
    "        print(f\"Query: {query}\")\n",
    "        query_set = set(tokenize(query))\n",
    "        \n",
    "        filenames = top_files(query_set, file_words, file_idfs, n=FILE_MATCHES)\n",
    "        \n",
    "        sentences = dict()\n",
    "        for filename in filenames:\n",
    "            for passage in files[filename].split(\"\\n\"):\n",
    "                for sentence in nltk.sent_tokenize(passage):\n",
    "                    tokens = tokenize(sentence)\n",
    "                    if tokens:\n",
    "                        sentences[sentence] = (tokens, filename)\n",
    "\n",
    "        idfs = compute_idfs({key: value[0] for key, value in sentences.items()})\n",
    "        \n",
    "        matches = top_sentences(query_set, {key: value[0] for key, value in sentences.items()}, idfs, n=SENTENCE_MATCHES)\n",
    "        for match in matches:\n",
    "            print(f\"Answer: {match}\")\n",
    "            print(f\"Source: {sentences[match][1]}\")\n",
    "        print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define queries for the question-answering system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = [\n",
    "    \"What are the types of supervised learning?\",\n",
    "    \"How do neurons connect in a neural network?\",\n",
    "    \"When was Python 3.0 released?\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specify the corpus directory and call the main function with the corpus directory and queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: What are the types of supervised learning?\n",
      "Answer: Types of supervised learning algorithms include Active learning , classification and regression.\n",
      "Source: machine_learning.txt\n",
      "\n",
      "\n",
      "Query: How do neurons connect in a neural network?\n",
      "Answer: Neurons of one layer connect only to neurons of the immediately preceding and immediately following layers.\n",
      "Source: neural_network.txt\n",
      "\n",
      "\n",
      "Query: When was Python 3.0 released?\n",
      "Answer: Python 3.0 was released on 3 December 2008.\n",
      "Source: python.txt\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "path_to_corpus_directory = 'corpus'\n",
    "# path_to_corpus_directory = 'corpus2'\n",
    "\n",
    "# Call the main function with the corpus directory and queries\n",
    "main(path_to_corpus_directory, queries)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".conda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
